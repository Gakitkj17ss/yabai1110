#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Attentionå¯è¦–åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆä¿®æ­£ç‰ˆï¼‰

ã€ä¿®æ­£å†…å®¹ã€‘
- ConsonantOnlyPhonemeEncoderã®äº’æ›æ€§ä¿®æ­£
- encode_text â†’ encode_phonemes ã¸ã®å¤‰æ›´
- text_to_phonemes ãƒ¡ã‚½ãƒƒãƒ‰ã®è¿½åŠ 
"""

import torch
import matplotlib.pyplot as plt
import numpy as np
from typing import Dict, List, Optional, Tuple
import os

# ===== PER helper (Levenshtein-based) =====
def _levenshtein_sdi(ref, hyp):
    """
    ref, hyp: list[str]ï¼ˆéŸ³ç´ åˆ—ï¼‰
    return (S, D, I)
    """
    n, m = len(ref), len(hyp)
    dp = [[0]*(m+1) for _ in range(n+1)]
    bt = [[0]*(m+1) for _ in range(n+1)]  # 0:diag, 1:up(del), 2:left(ins)
    for i in range(1, n+1):
        dp[i][0] = i; bt[i][0] = 1
    for j in range(1, m+1):
        dp[0][j] = j; bt[0][j] = 2
    for i in range(1, n+1):
        for j in range(1, m+1):
            cost = 0 if ref[i-1] == hyp[j-1] else 1
            a = dp[i-1][j-1] + cost
            b = dp[i-1][j] + 1
            c = dp[i][j-1] + 1
            if a <= b and a <= c:
                dp[i][j] = a; bt[i][j] = 0
            elif b <= c:
                dp[i][j] = b; bt[i][j] = 1
            else:
                dp[i][j] = c; bt[i][j] = 2
    i, j = n, m
    S = D = I = 0
    while i > 0 or j > 0:
        code = bt[i][j]
        if i > 0 and j > 0 and code == 0:
            if ref[i-1] != hyp[j-1]:
                S += 1
            i -= 1; j -= 1
        elif i > 0 and (j == 0 or code == 1):
            D += 1; i -= 1
        else:
            I += 1; j -= 1
    return S, D, I

def _sequence_per_percent(ref, hyp):
    """PER[%] = (S+D+I)/N * 100"""
    S, D, I = _levenshtein_sdi(ref, hyp)
    N = max(1, len(ref))
    return 100.0 * (S + D + I) / N
# ===== end helper =====


class AttentionVisualizer:
    """Attentioné‡ã¿å¯è¦–åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model, phoneme_encoder, device='cuda'):
        """
        Args:
            model: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
            phoneme_encoder: éŸ³ç´ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
            device: ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹
        """
        self.model = model
        self.phoneme_encoder = phoneme_encoder
        self.device = device
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«
        self.model.eval()
    
    def _text_to_phonemes(self, text) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³ç´ åˆ—ã«å¤‰æ›
        
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ–‡å­—åˆ—ã¾ãŸã¯ãƒªã‚¹ãƒˆï¼‰
        
        Returns:
            éŸ³ç´ ã®ãƒªã‚¹ãƒˆ
        """
        # 0. ã™ã§ã«éŸ³ç´ åˆ—ï¼ˆãƒªã‚¹ãƒˆï¼‰ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        if isinstance(text, list):
            return text
        
        # phoneme_encoderã®ç¨®é¡ã«å¿œã˜ã¦å‡¦ç†ã‚’åˆ†å²
        
        # 1. encode_textãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚‹å ´åˆ
        if hasattr(self.phoneme_encoder, 'encode_text'):
            return self.phoneme_encoder.encode_text(text)
        
        # 2. text_to_phonemesãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚‹å ´åˆï¼ˆæ–‡å­—åˆ—ã®ã¿å—ã‘ä»˜ã‘ã‚‹ï¼‰
        if hasattr(self.phoneme_encoder, 'text_to_phonemes'):
            if isinstance(text, str):
                return self.phoneme_encoder.text_to_phonemes(text)
        
        # 3. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æ–‡å­—ã‚’åˆ†å‰²ã—ã¦éŸ³ç´ ã¨ã—ã¦æ‰±ã†
        if isinstance(text, str):
            print(f"Warning: phoneme_encoderã«éŸ³ç´ å¤‰æ›ãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æ–‡å­—åˆ—ã‚’åˆ†å‰²ã—ã¾ã™: {text}")
            return list(text)
        
        # 4. ãã®ä»–ã®å‹ã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼
        raise TypeError(f"text must be str or list, got {type(text)}")
    
    def _phonemes_to_ids(self, phonemes: List[str]) -> List[int]:
        """
        éŸ³ç´ åˆ—ã‚’IDã«å¤‰æ›
        
        Args:
            phonemes: éŸ³ç´ ã®ãƒªã‚¹ãƒˆ
        
        Returns:
            éŸ³ç´ IDã®ãƒªã‚¹ãƒˆ
        """
        # encode_phonemesãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚‹å ´åˆ
        if hasattr(self.phoneme_encoder, 'encode_phonemes'):
            return self.phoneme_encoder.encode_phonemes(phonemes)
        
        # phoneme_to_idãŒã‚ã‚‹å ´åˆ
        if hasattr(self.phoneme_encoder, 'phoneme_to_id'):
            return [self.phoneme_encoder.phoneme_to_id.get(p, 0) for p in phonemes]
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: id2phonemeã®é€†å¼•ã
        if hasattr(self.phoneme_encoder, 'id2phoneme'):
            phoneme_to_id = {v: k for k, v in self.phoneme_encoder.id2phoneme.items()}
            return [phoneme_to_id.get(p, 0) for p in phonemes]
        
        raise AttributeError("phoneme_encoderã«éŸ³ç´ â†’IDå¤‰æ›ãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
    
    def visualize_attention_with_evaluation(
        self,
        video: torch.Tensor,
        text,  # str or List[str]
        save_path: Optional[str] = None,
        evaluator=None
    ) -> Dict:
        """
        Attentioné‡ã¿ã‚’å¯è¦–åŒ–ã—ã€äºˆæ¸¬ã‚’è©•ä¾¡
        
        Args:
            video: å‹•ç”»ãƒ†ãƒ³ã‚½ãƒ« [T, C, H, W]
            text: æ­£è§£ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ–‡å­—åˆ—ï¼‰ã¾ãŸã¯éŸ³ç´ åˆ—ï¼ˆãƒªã‚¹ãƒˆï¼‰
            save_path: ä¿å­˜å…ˆãƒ‘ã‚¹
            evaluator: è©•ä¾¡å™¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            çµæœè¾æ›¸
        """
        self.model.eval()
        
        with torch.no_grad():
            # ãƒãƒƒãƒæ¬¡å…ƒã‚’è¿½åŠ 
            if video.dim() == 4:
                video = video.unsqueeze(0)  # [1, T, C, H, W]
            
            video = video.to(self.device)
            
            # é †ä¼æ’­ï¼ˆAttention weightsã‚’å–å¾—ï¼‰
            # ãƒ¢ãƒ‡ãƒ«ãŒreturn_attentionã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã‹ç¢ºèª
            try:
                outputs = self.model(video, return_attention=True)
                
                # outputsãŒã‚¿ãƒ—ãƒ«ã®å ´åˆï¼ˆoutputs, attention_weightsï¼‰
                if isinstance(outputs, tuple) and len(outputs) == 2:
                    outputs, attention_weights = outputs
                    attention_weights = attention_weights.cpu().numpy()
                    print(f"  âœ“ Attention weights from return_attention=True: {attention_weights.shape}")
                else:
                    # return_attentionã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„å ´åˆ
                    attention_weights = None
                    print(f"  âš  Model returned unexpected format with return_attention=True")
                    
            except TypeError:
                # return_attentionãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„
                print(f"  âš  Model does not support return_attention parameter")
                outputs = self.model(video)
                attention_weights = None
            
            # è¿½åŠ ã®Attention weightså–å¾—æ–¹æ³•
            if attention_weights is None:
                # æ–¹æ³•1: model.attention_weightså±æ€§
                if hasattr(self.model, 'attention_weights') and self.model.attention_weights is not None:
                    attention_weights = self.model.attention_weights.cpu().numpy()
                    print(f"  âœ“ Attention weights from model.attention_weights: {attention_weights.shape}")
                
                # æ–¹æ³•2: outputsè¾æ›¸ã«attention_weightsãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
                elif isinstance(outputs, dict) and 'attention_weights' in outputs:
                    attention_weights = outputs['attention_weights'].cpu().numpy()
                    print(f"  âœ“ Attention weights from outputs dict: {attention_weights.shape}")
                    outputs = outputs['logits']  # logitsã‚’å–ã‚Šå‡ºã™
                
                # æ–¹æ³•3: model.get_attention_weights()ãƒ¡ã‚½ãƒƒãƒ‰
                elif hasattr(self.model, 'get_attention_weights'):
                    attention_weights = self.model.get_attention_weights().cpu().numpy()
                    print(f"  âœ“ Attention weights from get_attention_weights(): {attention_weights.shape}")
                
                # æ–¹æ³•4: æœ€å¾Œã®forward passã®çµæœã‚’ä¿å­˜ã—ã¦ã„ã‚‹å ´åˆ
                elif hasattr(self.model, 'last_attention_weights'):
                    attention_weights = self.model.last_attention_weights.cpu().numpy()
                    print(f"  âœ“ Attention weights from last_attention_weights: {attention_weights.shape}")
                
                else:
                    print(f"  âš  Attention weights not found in model")
                    print(f"     Available model methods: {[m for m in dir(self.model) if not m.startswith('_') and 'forward' in m.lower()]}")
            
            # outputsãŒã‚¿ãƒ—ãƒ«ã®å ´åˆ
            if isinstance(outputs, tuple):
                outputs = outputs[0]
            
            # äºˆæ¸¬ã‚’å–å¾—
            log_probs = torch.nn.functional.log_softmax(outputs, dim=-1)
            log_probs = log_probs.permute(1, 0, 2)  # [T, 1, num_classes]
            
            _, max_indices = torch.max(log_probs, dim=2)  # [T, 1]
            max_indices = max_indices.squeeze(1).cpu().numpy()  # [T]
            
            # CTC collapse
            pred_ids = []
            prev_id = None
            for idx in max_indices:
                if idx != self.phoneme_encoder.blank_id and idx != prev_id:
                    pred_ids.append(int(idx))
                prev_id = idx
            
            # ãƒ‡ã‚³ãƒ¼ãƒ‰
            pred_phonemes = self.phoneme_encoder.decode_phonemes(pred_ids)
            
            # æ­£è§£éŸ³ç´ ã‚’å–å¾—
            target_phonemes = self._text_to_phonemes(text)
            
            # è©•ä¾¡
            is_correct = (pred_phonemes == target_phonemes)
            
            result = {
                'predicted': pred_phonemes,
                'target': target_phonemes,
                'is_correct': is_correct,
                'attention_weights': attention_weights
            }
            
            # è©•ä¾¡å™¨ãŒã‚ã‚‹å ´åˆã¯è©³ç´°è©•ä¾¡
            if evaluator is not None:
                try:
                    eval_result = evaluator.evaluate_single(pred_phonemes, target_phonemes)
                    result.update(eval_result)
                except Exception as e:
                    print(f"âš  è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            
            # å¯è¦–åŒ–
            if save_path:
                self._plot_attention(
                    video=video.squeeze(0).cpu().numpy(),
                    attention_weights=attention_weights,
                    pred_phonemes=pred_phonemes,
                    target_phonemes=target_phonemes,
                    is_correct=is_correct,
                    save_path=save_path
                )
        
        return result
    
    def _plot_attention(
        self,
        video: np.ndarray,
        attention_weights: Optional[np.ndarray],
        pred_phonemes: List[str],
        target_phonemes: List[str],
        is_correct: bool,
        save_path: str
    ):
        """
        Attentioné‡ã¿ã‚’ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæ”¹å–„ç‰ˆï¼‰
        
        Args:
            video: å‹•ç”»ãƒ‡ãƒ¼ã‚¿ [T, C, H, W]
            attention_weights: Attentioné‡ã¿ [1, T] or [T] or None
            pred_phonemes: äºˆæ¸¬éŸ³ç´ 
            target_phonemes: æ­£è§£éŸ³ç´ 
            is_correct: æ­£è§£ã‹ã©ã†ã‹
            save_path: ä¿å­˜å…ˆãƒ‘ã‚¹
        """
        num_frames = video.shape[0]
        
        # Attention weightsã®ç¢ºèªã¨ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        has_attention = attention_weights is not None
        if has_attention:
            weights = attention_weights.squeeze()
            print(f"  ğŸ“Š Attentionçµ±è¨ˆ:")
            print(f"     Shape: {attention_weights.shape} â†’ {weights.shape}")
            print(f"     Min: {weights.min():.6f}, Max: {weights.max():.6f}")
            print(f"     Mean: {weights.mean():.6f}, Std: {weights.std():.6f}")
            print(f"     Range: {weights.max() - weights.min():.6f}")
            
            # ãƒ”ãƒ¼ã‚¯æƒ…å ±
            peak_idx = np.argmax(weights)
            print(f"     Peak: Frame {peak_idx} (weight={weights[peak_idx]:.6f})")
            
            # æ³¨ç›®åº¦ã®åˆ†å¸ƒ
            top_5_indices = np.argsort(weights)[-5:][::-1]
            print(f"     Top 5 frames: {top_5_indices.tolist()}")
            
            # ç¯„å›²ãŒç‹­ã„å ´åˆã¯è­¦å‘Š
            weight_range = weights.max() - weights.min()
            if weight_range < 0.1:
                print(f"     âš ï¸  æ³¨æ„: Attentioné‡ã¿ã®ç¯„å›²ãŒç‹­ã„ ({weight_range:.6f})")
                print(f"         â†’ AttentionãŒã»ã¼å‡ä¸€ã§ã€é¸æŠçš„ã«æ³¨ç›®ã§ãã¦ã„ãªã„å¯èƒ½æ€§")
                print(f"         â†’ Temperature ã‚’ä¸‹ã’ã‚‹ (ä¾‹: 0.5) ã‹ã€softmax ã«å¤‰æ›´ã‚’æ¤œè¨")
        else:
            print(f"  âš  Attention weights not available")
        
        # å›³ã®ä½œæˆ
        if has_attention:
            fig = plt.figure(figsize=(14, 10))
            gs = fig.add_gridspec(3, 1, height_ratios=[2, 1, 1], hspace=0.3)
            ax_video = fig.add_subplot(gs[0])
            ax_attention = fig.add_subplot(gs[1])
            ax_heatmap = fig.add_subplot(gs[2])
        else:
            fig, ax_video = plt.subplots(1, 1, figsize=(14, 4))
            ax_attention = None
            ax_heatmap = None
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚µãƒ ãƒã‚¤ãƒ«è¡¨ç¤º
        num_display = min(10, num_frames)
        indices = np.linspace(0, num_frames - 1, num_display, dtype=int)
        
        thumbnails = []
        for idx in indices:
            frame = video[idx]
            # ãƒãƒ£ãƒ³ãƒãƒ«ãŒæœ€åˆã®å ´åˆã¯æœ€å¾Œã«ç§»å‹• [C, H, W] -> [H, W, C]
            if frame.shape[0] in [1, 3]:
                frame = np.transpose(frame, (1, 2, 0))
            # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã®å ´åˆ
            if frame.shape[-1] == 1:
                frame = frame.squeeze(-1)
            thumbnails.append(frame)
        
        # ã‚µãƒ ãƒã‚¤ãƒ«çµåˆ
        thumbnail_strip = np.concatenate(thumbnails, axis=1)
        
        # æ­£è¦åŒ–
        if thumbnail_strip.max() > 1.0:
            thumbnail_strip = thumbnail_strip / 255.0
        
        ax_video.imshow(thumbnail_strip, cmap='gray' if len(thumbnail_strip.shape) == 2 else None)
        ax_video.set_title(
            f"{'âœ“ æ­£è§£' if is_correct else 'âœ— ä¸æ­£è§£'}\n"
            f"äºˆæ¸¬: {' '.join(pred_phonemes)}\n"
            f"æ­£è§£: {' '.join(target_phonemes)}",
            fontsize=12,
            fontweight='bold',
            color='green' if is_correct else 'red'
        )
        ax_video.axis('off')
        
        # Attentioné‡ã¿ã®ãƒ—ãƒ­ãƒƒãƒˆ
        if ax_attention is not None and has_attention:
            weights = attention_weights.squeeze()  # [T]
            
            # æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•
            frames_idx = np.arange(len(weights))
            ax_attention.plot(frames_idx, weights, linewidth=2.5, color='#2E86DE', marker='o', markersize=4)
            ax_attention.fill_between(frames_idx, weights, alpha=0.3, color='#54A0FF')
            
            # ãƒ”ãƒ¼ã‚¯ä½ç½®ã«å°
            peak_idx = np.argmax(weights)
            ax_attention.scatter([peak_idx], [weights[peak_idx]], 
                               color='red', s=100, zorder=5, marker='*', 
                               label=f'Peak at frame {peak_idx}')
            
            ax_attention.set_xlabel('Frame Index', fontsize=11, fontweight='bold')
            ax_attention.set_ylabel('Attention Weight', fontsize=11, fontweight='bold')
            ax_attention.set_title('Attention Weights over Time (Line Plot)', fontsize=11, fontweight='bold')
            ax_attention.grid(True, alpha=0.3, linestyle='--')
            ax_attention.legend(loc='upper right')
            ax_attention.set_xlim(-0.5, len(weights) - 0.5)
            
            # Yè»¸ã®ç¯„å›²ã‚’èª¿æ•´
            y_min, y_max = weights.min(), weights.max()
            y_range = y_max - y_min
            ax_attention.set_ylim(y_min - 0.1 * y_range, y_max + 0.1 * y_range)
        
        # Attentioné‡ã¿ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        if ax_heatmap is not None and has_attention:
            weights = attention_weights.squeeze()  # [T]
            weights_2d = weights.reshape(1, -1)  # [1, T]
            
            im = ax_heatmap.imshow(weights_2d, cmap='hot', aspect='auto', interpolation='nearest')
            ax_heatmap.set_xlabel('Frame Index', fontsize=11, fontweight='bold')
            ax_heatmap.set_ylabel('Attention', fontsize=11, fontweight='bold')
            ax_heatmap.set_title('Attention Weights Heatmap', fontsize=11, fontweight='bold')
            ax_heatmap.set_yticks([])
            
            # ã‚«ãƒ©ãƒ¼ãƒãƒ¼
            cbar = plt.colorbar(im, ax=ax_heatmap, orientation='horizontal', pad=0.1, fraction=0.05)
            cbar.set_label('Weight', fontsize=10)
        
        plt.tight_layout()
        
        # ä¿å­˜
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ“ Saved: {save_path}")


def visualize_attention_with_samples(
    model,
    data_loader,
    phoneme_encoder,
    device='cuda',
    num_samples=5,
    save_dir='results/attention_visualization',
    evaluator=None
) -> Dict:
    """
    è¤‡æ•°ã‚µãƒ³ãƒ—ãƒ«ã§Attentionå¯è¦–åŒ–ï¼ˆä¸æ­£è§£ã‚µãƒ³ãƒ—ãƒ«ã« PER[%] ã‚’ä»˜ä¸ã—ã€ãƒ¯ãƒ¼ã‚¹ãƒˆTOP10ã‚’è¡¨ç¤º/ä¿å­˜ï¼‰
    """

    # --- PER helpers (ãƒ­ãƒ¼ã‚«ãƒ«å®šç¾©ï¼šå¤–éƒ¨ä¾å­˜ãªã—) ---
    def _levenshtein_sdi(ref, hyp):
        """ref/hyp: list[str] -> (S,D,I)"""
        n, m = len(ref), len(hyp)
        dp = [[0]*(m+1) for _ in range(n+1)]
        bt = [[0]*(m+1) for _ in range(n+1)]  # 0:diag, 1:up(del), 2:left(ins)
        for i in range(1, n+1):
            dp[i][0] = i; bt[i][0] = 1
        for j in range(1, m+1):
            dp[0][j] = j; bt[0][j] = 2
        for i in range(1, n+1):
            for j in range(1, m+1):
                cost = 0 if ref[i-1] == hyp[j-1] else 1
                a = dp[i-1][j-1] + cost
                b = dp[i-1][j] + 1
                c = dp[i][j-1] + 1
                if a <= b and a <= c:
                    dp[i][j] = a; bt[i][j] = 0
                elif b <= c:
                    dp[i][j] = b; bt[i][j] = 1
                else:
                    dp[i][j] = c; bt[i][j] = 2
        i, j = n, m
        S = D = I = 0
        while i > 0 or j > 0:
            code = bt[i][j]
            if i > 0 and j > 0 and code == 0:
                if ref[i-1] != hyp[j-1]:
                    S += 1
                i -= 1; j -= 1
            elif i > 0 and (j == 0 or code == 1):
                D += 1; i -= 1
            else:
                I += 1; j -= 1
        return S, D, I

    def _per_percent(ref, hyp):
        """PER[%] = (S+D+I)/len(ref)*100"""
        S, D, I = _levenshtein_sdi(ref, hyp)
        N = max(1, len(ref))
        return 100.0 * (S + D + I) / N, (S, D, I)
    # --- end helpers ---

    os.makedirs(save_dir, exist_ok=True)
    visualizer = AttentionVisualizer(model, phoneme_encoder, device)

    # è©•ä¾¡å™¨
    if evaluator is None:
        try:
            from matrics_undefined import CTCAwareEvaluator
            evaluator = CTCAwareEvaluator()
        except ImportError:
            print("âš  CTCAwareEvaluator not found. Using simple evaluation.")
            evaluator = None

    correct_samples = []
    incorrect_samples = []

    total_samples = 0
    correct_count = 0

    # ã‚µãƒ³ãƒ—ãƒ«åé›†
    for batch in data_loader:
        videos = batch['video']
        targets = batch['target']
        target_lengths = batch['target_length']

        batch_size = videos.size(0)
        target_offset = 0

        for i in range(batch_size):
            if total_samples >= num_samples * 2:  # æ­£è§£ãƒ»ä¸æ­£è§£ãã‚Œãã‚Œ num_samples ç›®æ¨™
                break

            video = videos[i]
            target_len = int(target_lengths[i].item())
            target_ids = targets[target_offset:target_offset + target_len].cpu().numpy()
            target_phonemes = phoneme_encoder.decode_phonemes(target_ids)

            save_path = os.path.join(save_dir, f'sample_{total_samples:03d}.png')
            result = visualizer.visualize_attention_with_evaluation(
                video=video,
                text=target_phonemes,  # éŸ³ç´ åˆ—ã‚’ç›´æ¥æ¸¡ã™
                save_path=save_path,
                evaluator=evaluator
            )

            # PER è¨ˆç®—
            per, (S, D, I) = _per_percent(result['target'], result['predicted'])

            # çµæœä¿å­˜
            sample_info = {
                'sample_id': total_samples,
                'predicted': result['predicted'],
                'target': result['target'],
                'is_correct': result['is_correct'],
                'save_path': save_path,
                'per': round(per, 2),
                'S': int(S), 'D': int(D), 'I': int(I),
            }

            if result['is_correct']:
                correct_samples.append(sample_info)
                correct_count += 1
            else:
                incorrect_samples.append(sample_info)

            total_samples += 1
            target_offset += target_len

        if total_samples >= num_samples * 2:
            break

    # ä¸æ­£è§£ã‚’ PER é™é †ï¼ˆæ‚ªã„é †ï¼‰ã§ã‚½ãƒ¼ãƒˆ
    incorrect_samples.sort(key=lambda s: -s['per'])

    # ã‚µãƒãƒªãƒ¼
    accuracy = correct_count / total_samples if total_samples > 0 else 0.0
    result = {
        'total_samples': total_samples,
        'correct_count': correct_count,
        'accuracy': accuracy,
        'correct_samples': correct_samples,
        'incorrect_samples': incorrect_samples,
        'save_dir': save_dir
    }

    # ===== ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ› =====
    print(f"\n{'='*70}")
    print(f"Attentionå¯è¦–åŒ– + ã‚µãƒ³ãƒ—ãƒ«è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ")
    print(f"{'='*70}")

    print(f"\nã€å…¨ä½“çµ±è¨ˆã€‘")
    print(f"  ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {total_samples}")
    print(f"  æ­£è§£æ•°: {correct_count}")
    print(f"  ä¸æ­£è§£æ•°: {total_samples - correct_count}")
    print(f"  ç²¾åº¦: {accuracy*100:.1f}%")
    print(f"  ä¿å­˜å…ˆ: {save_dir}")

    # æ­£è§£ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€å¤§5ä»¶ï¼‰
    if correct_samples:
        print(f"\nã€æ­£è§£ã‚µãƒ³ãƒ—ãƒ«ã€‘ ({min(5, len(correct_samples))}ä»¶)")
        for i, s in enumerate(correct_samples[:5], 1):
            pred_str = ' '.join(s['predicted'])
            tgt_str  = ' '.join(s['target'])
            print(f"  {i}. âœ“ PER={s['per']:.2f}%  äºˆæ¸¬={pred_str}, æ­£è§£={tgt_str}")
            print(f"     ãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(s['save_path'])}")

    # ä¸æ­£è§£ã‚µãƒ³ãƒ—ãƒ«ï¼ˆTOP10ï¼‰
    if incorrect_samples:
        topn = min(10, len(incorrect_samples))
        print(f"\nã€ä¸æ­£è§£ã‚µãƒ³ãƒ—ãƒ«ã€‘ (TOP {topn})")
        for i, s in enumerate(incorrect_samples[:topn], 1):
            pred_str = ' '.join(s['predicted'])
            tgt_str  = ' '.join(s['target'])
            print(f"  {i}. âœ— PER={s['per']:.2f}%  äºˆæ¸¬={pred_str}, æ­£è§£={tgt_str}")
            print(f"     ãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(s['save_path'])}")
            # ã‚¨ãƒ©ãƒ¼åˆ†æï¼ˆé›†åˆå·®ï¼‰
            missing = set(s['target']) - set(s['predicted'])
            extra   = set(s['predicted']) - set(s['target'])
            if missing:
                print(f"     æ¬ è½éŸ³ç´ : {missing}")
            if extra:
                print(f"     ä½™åˆ†éŸ³ç´ : {extra}")

    # Attentionçµ±è¨ˆï¼ˆæ¡ˆå†…ï¼‰
    if total_samples > 0:
        print(f"\nã€Attentionçµ±è¨ˆã€‘")
        print(f"  å¯è¦–åŒ–ç”»åƒã‚’ç¢ºèªã—ã¦ãã ã•ã„: {save_dir}")
        print(f"  æ³¨ç›®åº¦ãŒé«˜ã„ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç¢ºèªã§ãã¾ã™")
    print(f"{'='*70}\n")

    # ===== JSON ä¿å­˜ï¼ˆPERã¨SDIã‚’å«ã‚€ï¼‰=====
    import json
    to_json = {
        'summary': {
            'total_samples': total_samples,
            'correct_count': correct_count,
            'incorrect_count': total_samples - correct_count,
            'accuracy': accuracy
        },
        'correct_samples': [
            {
                'sample_id': s['sample_id'],
                'predicted': s['predicted'],
                'target': s['target'],
                'file': os.path.basename(s['save_path']),
                'per': s['per'], 'S': s['S'], 'D': s['D'], 'I': s['I'],
            } for s in correct_samples
        ],
        'incorrect_samples': [
            {
                'sample_id': s['sample_id'],
                'predicted': s['predicted'],
                'target': s['target'],
                'file': os.path.basename(s['save_path']),
                'per': s['per'], 'S': s['S'], 'D': s['D'], 'I': s['I'],
                'missing_phonemes': list(set(s['target']) - set(s['predicted'])),
                'extra_phonemes': list(set(s['predicted']) - set(s['target'])),
            } for s in incorrect_samples
        ]
    }
    json_path = os.path.join(save_dir, 'evaluation_results.json')
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(to_json, f, indent=2, ensure_ascii=False)
    print(f"âœ“ è©•ä¾¡çµæœã‚’JSONä¿å­˜: {json_path}\n")

    return result


if __name__ == "__main__":
    print("Attentionå¯è¦–åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆä¿®æ­£ç‰ˆï¼‰")