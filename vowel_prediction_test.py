#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯éŸ³ã®ã¿ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿéš›ã«äºˆæ¸¬ã§ãã‚‹ã‹ãƒ†ã‚¹ãƒˆ
"""

import torch
import torch.nn as nn
from phoneme_encoder import JapanesePhonemeEncoder
from model import HybridCTCLipReadingModel
from config import Config

def test_model_creation():
    """ãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ãä½œæˆã•ã‚Œã‚‹ã‹ãƒ†ã‚¹ãƒˆ"""
    print("=== ãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ†ã‚¹ãƒˆ ===")
    
    # æ¯éŸ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
    phoneme_encoder = JapanesePhonemeEncoder(vowel_only=True)
    print(f"èªå½™æ•°: {phoneme_encoder.vocab_size}")
    print(f"éŸ³éŸ»: {phoneme_encoder.phonemes}")
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = HybridCTCLipReadingModel(num_phonemes=phoneme_encoder.vocab_size)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ç¢ºèª
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}")
    
    return model, phoneme_encoder

def test_forward_pass():
    """é †ä¼æ’­ãƒ†ã‚¹ãƒˆ"""
    print("\n=== é †ä¼æ’­ãƒ†ã‚¹ãƒˆ ===")
    
    model, phoneme_encoder = test_model_creation()
    model.eval()
    
    # ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆ (ãƒãƒƒãƒ=2, æ™‚é–“=10, ãƒãƒ£ãƒ³ãƒãƒ«=1, é«˜ã•=96, å¹…=96)
    batch_size = 2
    seq_len = 10
    dummy_input = torch.randn(batch_size, seq_len, 1, 96, 96)
    
    print(f"å…¥åŠ›å½¢çŠ¶: {dummy_input.shape}")
    
    with torch.no_grad():
        output = model(dummy_input)
    
    print(f"å‡ºåŠ›å½¢çŠ¶: {output.shape}")
    print(f"æœŸå¾…å½¢çŠ¶: (batch={batch_size}, time={seq_len}, classes={phoneme_encoder.vocab_size})")
    
    # å‡ºåŠ›ã®å†…å®¹ç¢ºèª
    print(f"å‡ºåŠ›å€¤ç¯„å›²: {output.min().item():.3f} ~ {output.max().item():.3f}")
    
    # CTCç”¨ã«log_softmaxã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    print("å„æ™‚åˆ»ã®ç¢ºç‡å’Œï¼ˆlogç©ºé–“ï¼‰:")
    for t in range(min(3, seq_len)):
        prob_sum = torch.exp(output[0, t]).sum().item()
        print(f"  æ™‚åˆ»{t}: {prob_sum:.3f} (â‰ˆ1.0ãŒæ­£å¸¸)")
    
    return model, phoneme_encoder, output

def test_ctc_decoding():
    """CTC ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
    print("\n=== CTC ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ ===")
    
    model, phoneme_encoder, output = test_forward_pass()
    
    # æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã§ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    log_probs = output[0]  # (time, num_classes)
    
    # Greedy decoding
    pred_seq = torch.argmax(log_probs, dim=-1)
    print(f"ç”Ÿäºˆæ¸¬ID: {pred_seq.tolist()}")
    
    # CTC decoding (é‡è¤‡ãƒ»BLANKé™¤å»)
    decoded_seq = []
    prev_token = -1
    
    for token in pred_seq:
        token = token.item()
        if token != prev_token and token != 0:  # 0ã¯BLANK
            decoded_seq.append(token)
        prev_token = token
    
    print(f"CTCå¾ŒID: {decoded_seq}")
    
    # éŸ³éŸ»ã«å¤‰æ›
    decoded_phonemes = phoneme_encoder.decode_phonemes(decoded_seq)
    result_text = ''.join(decoded_phonemes)
    
    print(f"äºˆæ¸¬éŸ³éŸ»: {decoded_phonemes}")
    print(f"äºˆæ¸¬ãƒ†ã‚­ã‚¹ãƒˆ: '{result_text}'")
    
    # å„éŸ³éŸ»ã®ç¢ºç‡ã‚‚è¡¨ç¤º
    print("\nå„æ™‚åˆ»ã®éŸ³éŸ»ç¢ºç‡:")
    for t in range(min(5, log_probs.size(0))):
        probs = torch.exp(log_probs[t])
        top_prob, top_idx = torch.max(probs, dim=0)
        top_phoneme = phoneme_encoder.id_to_phoneme[top_idx.item()]
        print(f"  æ™‚åˆ»{t}: {top_phoneme} ({top_prob.item():.3f})")

def test_training_compatibility():
    """å­¦ç¿’ã¨ã®äº’æ›æ€§ãƒ†ã‚¹ãƒˆ"""
    print("\n=== å­¦ç¿’äº’æ›æ€§ãƒ†ã‚¹ãƒˆ ===")
    
    model, phoneme_encoder = test_model_creation()
    
    # ãƒ€ãƒŸãƒ¼å­¦ç¿’ãƒ‡ãƒ¼ã‚¿
    videos = torch.randn(2, 8, 1, 96, 96)  # ãƒãƒƒãƒ=2, æ™‚é–“=8
    
    # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    texts = ["ã“ã‚“ã«ã¡ã¯", "ã‚ã‚ŠãŒã¨ã†"]
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ
    all_targets = []
    target_lengths = []
    
    print("å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å¤‰æ›:")
    for text in texts:
        phonemes = phoneme_encoder.text_to_phonemes(text)
        phoneme_ids = phoneme_encoder.encode_phonemes(phonemes)
        
        all_targets.extend(phoneme_ids)
        target_lengths.append(len(phoneme_ids))
        
        print(f"  '{text}' â†’ {phonemes} â†’ {phoneme_ids}")
    
    targets = torch.tensor(all_targets, dtype=torch.long)
    input_lengths = torch.tensor([8, 8], dtype=torch.long)  # å‹•ç”»é•·
    target_lengths = torch.tensor(target_lengths, dtype=torch.long)
    
    print(f"\nã‚¿ãƒ¼ã‚²ãƒƒãƒˆå½¢çŠ¶: {targets.shape}")
    print(f"å…¥åŠ›é•·: {input_lengths}")
    print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé•·: {target_lengths}")
    
    # CTCæå¤±ãƒ†ã‚¹ãƒˆ
    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)
    
    with torch.no_grad():
        outputs = model(videos)  # (batch, time, classes)
        outputs_transposed = outputs.transpose(0, 1)  # (time, batch, classes)
        
        try:
            loss = criterion(outputs_transposed, targets, input_lengths, target_lengths)
            print(f"CTCæå¤±: {loss.item():.4f}")
            print("âœ… å­¦ç¿’æº–å‚™å®Œäº†")
        except Exception as e:
            print(f"âŒ CTCæå¤±ã‚¨ãƒ©ãƒ¼: {e}")

def test_vowel_only_effectiveness():
    """æ¯éŸ³ã®ã¿ãƒ¢ãƒ¼ãƒ‰ã®åŠ¹æœç¢ºèª"""
    print("\n=== æ¯éŸ³ã®ã¿ãƒ¢ãƒ¼ãƒ‰åŠ¹æœç¢ºèª ===")
    
    # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰
    full_encoder = JapanesePhonemeEncoder(vowel_only=False)
    print(f"é€šå¸¸ãƒ¢ãƒ¼ãƒ‰èªå½™æ•°: {full_encoder.vocab_size}")
    
    # æ¯éŸ³ã®ã¿ãƒ¢ãƒ¼ãƒ‰  
    vowel_encoder = JapanesePhonemeEncoder(vowel_only=True)
    print(f"æ¯éŸ³ã®ã¿èªå½™æ•°: {vowel_encoder.vocab_size}")
    
    reduction = (1 - vowel_encoder.vocab_size / full_encoder.vocab_size) * 100
    print(f"èªå½™å‰Šæ¸›ç‡: {reduction:.1f}%")
    
    # å­¦ç¿’ã®é•ã„
    print("\nå­¦ç¿’ã¸ã®å½±éŸ¿:")
    print(f"  å‡ºåŠ›å±¤: {full_encoder.vocab_size} â†’ {vowel_encoder.vocab_size} ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³")
    print(f"  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‰Šæ¸›: ç´„{reduction:.0f}%")
    print(f"  éå­¦ç¿’ãƒªã‚¹ã‚¯: å¤§å¹…æ¸›å°‘")
    print(f"  åæŸé€Ÿåº¦: å¤§å¹…å‘ä¸ŠæœŸå¾…")

if __name__ == "__main__":
    print("ğŸ¯ æ¯éŸ³ã®ã¿ãƒ¢ãƒ¼ãƒ‰å‹•ä½œç¢ºèªãƒ†ã‚¹ãƒˆ")
    print("=" * 50)
    
    try:
        test_model_creation()
        test_forward_pass() 
        test_ctc_decoding()
        test_training_compatibility()
        test_vowel_only_effectiveness()
        
        print("\n" + "=" * 50)
        print("âœ… å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸï¼æ¯éŸ³ã®ã¿ãƒ¢ãƒ¼ãƒ‰ã¯æ­£å¸¸ã«å‹•ä½œã—ã¾ã™")
        print("ğŸ¯ å­¦ç¿’å®Ÿè¡Œã§æ¯éŸ³äºˆæ¸¬ãŒæœŸå¾…ã§ãã¾ã™")
        
    except Exception as e:
        print(f"\nâŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()