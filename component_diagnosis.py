#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ¢ãƒ‡ãƒ«å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè¨ºæ–­ãƒ„ãƒ¼ãƒ«
CNNã®ç‰¹å¾´æŠ½å‡ºã€LSTMã®ç³»åˆ—å‡¦ç†ã€CTCã®éŸ³éŸ»äºˆæ¸¬ã‚’å€‹åˆ¥ã«è¨ºæ–­
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from simple_model import SimpleLipReadingModel
from phoneme_encoder import JapanesePhonemeEncoder
from config import Config

class ComponentDiagnostics:
    """å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®è¨ºæ–­ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model, phoneme_encoder):
        self.model = model
        self.phoneme_encoder = phoneme_encoder
        
    def diagnose_cnn_features(self, video_data, visualize=True):
        """CNNç‰¹å¾´æŠ½å‡ºã®è¨ºæ–­"""
        print("ğŸ” CNNç‰¹å¾´æŠ½å‡ºè¨ºæ–­")
        
        self.model.eval()
        with torch.no_grad():
            # CNNç‰¹å¾´ã®ã¿æŠ½å‡º
            cnn_features = self.model.cnn(video_data)  # (batch, time, feature_dim)
            
            batch_size, time_steps, feature_dim = cnn_features.shape
            
            print(f"  å…¥åŠ›å½¢çŠ¶: {video_data.shape}")
            print(f"  CNNå‡ºåŠ›å½¢çŠ¶: {cnn_features.shape}")
            print(f"  ç‰¹å¾´ç¯„å›²: {cnn_features.min():.3f} ~ {cnn_features.max():.3f}")
            print(f"  ç‰¹å¾´å¹³å‡: {cnn_features.mean():.3f}")
            print(f"  ç‰¹å¾´åˆ†æ•£: {cnn_features.var():.3f}")
            
            # æ™‚é–“è»¸ã§ã®ç‰¹å¾´å¤‰åŒ–
            temporal_variance = cnn_features.var(dim=1).mean()  # æ™‚é–“è»¸ã®åˆ†æ•£
            print(f"  æ™‚é–“å¤‰åŒ–: {temporal_variance:.3f} (é«˜ã„ã»ã©å‹•çš„)")
            
            # ç‰¹å¾´ã®ç›¸é–¢ï¼ˆæœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ä»–ã®æ¯”è¼ƒï¼‰
            if time_steps > 1:
                first_frame = cnn_features[:, 0:1, :]  # (batch, 1, feature_dim)
                correlations = []
                for t in range(1, min(10, time_steps)):
                    frame_t = cnn_features[:, t:t+1, :]
                    corr = F.cosine_similarity(first_frame, frame_t, dim=-1).mean()
                    correlations.append(corr.item())
                
                avg_correlation = np.mean(correlations)
                print(f"  ãƒ•ãƒ¬ãƒ¼ãƒ ç›¸é–¢: {avg_correlation:.3f} (ä½ã„ã»ã©å¤‰åŒ–å¤§)")
            
            if visualize and time_steps >= 10:
                # ç‰¹å¾´ã®å¯è¦–åŒ–ï¼ˆæœ€åˆã®10ãƒ•ãƒ¬ãƒ¼ãƒ ã€æœ€åˆã®16æ¬¡å…ƒï¼‰
                plt.figure(figsize=(12, 8))
                
                plt.subplot(2, 2, 1)
                features_to_plot = cnn_features[0, :10, :16].cpu().numpy()
                plt.imshow(features_to_plot.T, aspect='auto', cmap='viridis')
                plt.title('CNN Features (first 16 dims, 10 frames)')
                plt.xlabel('Time')
                plt.ylabel('Feature Dimension')
                plt.colorbar()
                
                plt.subplot(2, 2, 2)
                feature_norms = torch.norm(cnn_features[0], dim=-1).cpu().numpy()
                plt.plot(feature_norms)
                plt.title('Feature Magnitude over Time')
                plt.xlabel('Time')
                plt.ylabel('L2 Norm')
                
                plt.subplot(2, 2, 3)
                plt.hist(cnn_features[0].flatten().cpu().numpy(), bins=50, alpha=0.7)
                plt.title('Feature Value Distribution')
                plt.xlabel('Feature Value')
                plt.ylabel('Count')
                
                plt.subplot(2, 2, 4)
                if len(correlations) > 0:
                    plt.plot(correlations)
                    plt.title('Frame-to-Frame Correlation')
                    plt.xlabel('Frame Offset')
                    plt.ylabel('Cosine Similarity')
                
                plt.tight_layout()
                plt.savefig('cnn_diagnosis.png', dpi=150)
                plt.show()
            
            return {
                'feature_shape': cnn_features.shape,
                'feature_range': (cnn_features.min().item(), cnn_features.max().item()),
                'feature_mean': cnn_features.mean().item(),
                'feature_var': cnn_features.var().item(),
                'temporal_variance': temporal_variance.item(),
                'avg_correlation': avg_correlation if time_steps > 1 else 1.0
            }
    
    def diagnose_lstm_processing(self, video_data):
        """LSTMç³»åˆ—å‡¦ç†ã®è¨ºæ–­"""
        print("ğŸ” LSTMç³»åˆ—å‡¦ç†è¨ºæ–­")
        
        self.model.eval()
        with torch.no_grad():
            # CNN â†’ LSTM
            cnn_features = self.model.cnn(video_data)
            lstm_features = self.model.rnn(cnn_features)
            
            print(f"  CNN â†’ LSTM: {cnn_features.shape} â†’ {lstm_features.shape}")
            
            # LSTMå‡ºåŠ›ã®åˆ†æ
            print(f"  LSTMå‡ºåŠ›ç¯„å›²: {lstm_features.min():.3f} ~ {lstm_features.max():.3f}")
            print(f"  LSTMå‡ºåŠ›å¹³å‡: {lstm_features.mean():.3f}")
            print(f"  LSTMå‡ºåŠ›åˆ†æ•£: {lstm_features.var():.3f}")
            
            # ç³»åˆ—ã®æ»‘ã‚‰ã‹ã•ï¼ˆéš£æ¥ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®å·®ï¼‰
            if lstm_features.size(1) > 1:
                frame_diffs = torch.diff(lstm_features, dim=1)  # (batch, time-1, feature)
                avg_diff = frame_diffs.abs().mean()
                print(f"  ç³»åˆ—æ»‘ã‚‰ã‹ã•: {avg_diff:.3f} (ä½ã„ã»ã©æ»‘ã‚‰ã‹)")
            
            # åŒæ–¹å‘æ€§ã®ç¢ºèªï¼ˆæœ€åˆã¨æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰
            if lstm_features.size(1) >= 10:
                first_frames = lstm_features[:, :5, :].mean(dim=1)  # æœ€åˆã®5ãƒ•ãƒ¬ãƒ¼ãƒ å¹³å‡
                last_frames = lstm_features[:, -5:, :].mean(dim=1)  # æœ€å¾Œã®5ãƒ•ãƒ¬ãƒ¼ãƒ å¹³å‡
                bidirectional_similarity = F.cosine_similarity(first_frames, last_frames, dim=-1).mean()
                print(f"  åŒæ–¹å‘æƒ…å ±çµ±åˆ: {bidirectional_similarity:.3f}")
            
            return {
                'lstm_shape': lstm_features.shape,
                'lstm_range': (lstm_features.min().item(), lstm_features.max().item()),
                'lstm_mean': lstm_features.mean().item(),
                'lstm_var': lstm_features.var().item(),
                'sequence_smoothness': avg_diff.item() if lstm_features.size(1) > 1 else 0.0
            }
    
    def diagnose_ctc_output(self, video_data, target_text=None):
        """CTCå‡ºåŠ›å±¤ã®è¨ºæ–­"""
        print("ğŸ” CTCå‡ºåŠ›å±¤è¨ºæ–­")
        
        self.model.eval()
        with torch.no_grad():
            # å®Œå…¨ãªå‰å‘ãè¨ˆç®—
            outputs = self.model(video_data)  # (batch, time, num_classes)
            
            batch_size, time_steps, num_classes = outputs.shape
            print(f"  CTCå‡ºåŠ›å½¢çŠ¶: {outputs.shape}")
            
            # Log probabilities â†’ probabilities
            probs = torch.exp(outputs)
            
            # å„ã‚¯ãƒ©ã‚¹ã®å¹³å‡ç¢ºç‡
            avg_class_probs = probs.mean(dim=(0, 1))
            print(f"  å„ã‚¯ãƒ©ã‚¹å¹³å‡ç¢ºç‡:")
            for i, prob in enumerate(avg_class_probs):
                phoneme = self.phoneme_encoder.id_to_phoneme.get(i, f'ID{i}')
                print(f"    {phoneme}: {prob:.3f}")
            
            # BLANKç¢ºç‡ã®åˆ†æ
            blank_prob = avg_class_probs[0].item()
            non_blank_prob = avg_class_probs[1:].sum().item()
            print(f"  BLANK vs éBLANK: {blank_prob:.3f} vs {non_blank_prob:.3f}")
            
            # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ†æ
            entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=-1)  # (batch, time)
            avg_entropy = entropy.mean()
            print(f"  å¹³å‡ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {avg_entropy:.3f} (é«˜ã„ã»ã©ä¸ç¢ºå®Ÿ)")
            
            # æœ€ã‚‚ç¢ºä¿¡åº¦ã®é«˜ã„ã‚¯ãƒ©ã‚¹
            max_classes = torch.argmax(probs, dim=-1)  # (batch, time)
            class_counts = torch.bincount(max_classes.flatten(), minlength=num_classes)
            most_frequent_class = torch.argmax(class_counts).item()
            most_frequent_phoneme = self.phoneme_encoder.id_to_phoneme.get(most_frequent_class, f'ID{most_frequent_class}')
            print(f"  æœ€é »å‡ºäºˆæ¸¬: {most_frequent_phoneme} ({class_counts[most_frequent_class].item()}å›)")
            
            # CTC decodingçµæœ
            pred_sequence = torch.argmax(outputs[0], dim=-1).cpu().numpy()
            decoded_sequence = []
            prev_token = -1
            for token in pred_sequence:
                if token != prev_token and token != 0:
                    decoded_sequence.append(token)
                prev_token = token
            
            pred_phonemes = self.phoneme_encoder.decode_phonemes(decoded_sequence)
            pred_text = ''.join(pred_phonemes)
            print(f"  CTCäºˆæ¸¬çµæœ: '{pred_text}'")
            
            if target_text:
                target_phonemes = self.phoneme_encoder.text_to_phonemes(target_text)
                target_text_converted = ''.join(target_phonemes)
                print(f"  æ­£è§£ãƒ†ã‚­ã‚¹ãƒˆ: '{target_text}' â†’ '{target_text_converted}'")
            
            return {
                'ctc_shape': outputs.shape,
                'blank_prob': blank_prob,
                'non_blank_prob': non_blank_prob,
                'avg_entropy': avg_entropy.item(),
                'predicted_text': pred_text,
                'class_probabilities': avg_class_probs.cpu().numpy().tolist()
            }
    
    def full_diagnosis(self, video_data, target_text=None):
        """å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®è¨ºæ–­"""
        print("="*60)
        print("ğŸ”¬ ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«è¨ºæ–­")
        print("="*60)
        
        cnn_results = self.diagnose_cnn_features(video_data, visualize=True)
        lstm_results = self.diagnose_lstm_processing(video_data)
        ctc_results = self.diagnose_ctc_output(video_data, target_text)
        
        # å•é¡Œã®ç‰¹å®š
        print("\nğŸ¯ å•é¡Œè¨ºæ–­:")
        
        # CNNå•é¡Œ
        if cnn_results['temporal_variance'] < 0.01:
            print("  âš ï¸  CNN: æ™‚é–“å¤‰åŒ–ãŒå°‘ãªã™ãã‚‹ï¼ˆç‰¹å¾´æŠ½å‡ºãŒä¸ååˆ†ï¼‰")
        if cnn_results['avg_correlation'] > 0.95:
            print("  âš ï¸  CNN: ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®å¤‰åŒ–ãŒå°‘ãªã™ãã‚‹")
        
        # LSTMå•é¡Œ  
        if lstm_results['sequence_smoothness'] > 1.0:
            print("  âš ï¸  LSTM: ç³»åˆ—ãŒä¸å®‰å®šï¼ˆéåº¦ãªå¤‰åŒ–ï¼‰")
        if lstm_results['lstm_var'] < 0.01:
            print("  âš ï¸  LSTM: å‡ºåŠ›ã®åˆ†æ•£ãŒå°ã•ã™ãã‚‹")
        
        # CTCå•é¡Œ
        if ctc_results['blank_prob'] > 0.5:
            print("  ğŸš¨ CTC: BLANKåé‡å•é¡Œï¼")
        if ctc_results['avg_entropy'] < 0.5:
            print("  âš ï¸  CTC: éåº¦ã«ç¢ºä¿¡çš„ï¼ˆå¤šæ§˜æ€§ä¸è¶³ï¼‰")
        if ctc_results['avg_entropy'] > 2.0:
            print("  âš ï¸  CTC: éåº¦ã«ä¸ç¢ºå®Ÿï¼ˆå­¦ç¿’ä¸è¶³ï¼‰")
        
        # æ¨å¥¨å¯¾ç­–
        print("\nğŸ’¡ æ¨å¥¨å¯¾ç­–:")
        if ctc_results['blank_prob'] > 0.5:
            print("  1. CTCãƒã‚¤ã‚¢ã‚¹ã‚’ã•ã‚‰ã«èª¿æ•´")
            print("  2. BLANK penaltyã‚’å¼·åŒ–")
            print("  3. å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹")
        if cnn_results['temporal_variance'] < 0.01:
            print("  4. CNNå­¦ç¿’ç‡ã‚’å€‹åˆ¥ã«ä¸Šã’ã‚‹")
            print("  5. ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’è¿½åŠ ")
        
        return {
            'cnn': cnn_results,
            'lstm': lstm_results, 
            'ctc': ctc_results
        }

def run_diagnosis_on_sample():
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§è¨ºæ–­å®Ÿè¡Œ"""
    # éŸ³éŸ»ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
    phoneme_encoder = JapanesePhonemeEncoder(vowel_only=True)
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model = SimpleLipReadingModel(phoneme_encoder.vocab_size)
    model = model.to(Config.DEVICE)
    
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§è¨ºæ–­
    dummy_video = torch.randn(1, 30, 1, 96, 96).to(Config.DEVICE)
    dummy_text = "ã“ã‚“ã«ã¡ã¯"
    
    # è¨ºæ–­å®Ÿè¡Œ
    diagnostics = ComponentDiagnostics(model, phoneme_encoder)
    results = diagnostics.full_diagnosis(dummy_video, dummy_text)
    
    return results

if __name__ == "__main__":
    results = run_diagnosis_on_sample()
    print("\nè¨ºæ–­å®Œäº†ï¼")