#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èª­å”‡è¡“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Œå…¨è‡ªå‹•å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆéŸ³å£°åˆ†å‰²æ©Ÿèƒ½è¿½åŠ ç‰ˆï¼‰
å…¨ã¦ã®å‡¦ç†ã‚’ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«1ã¤ã§å®Œçµ:
1. ãƒšã‚¢CSVã‚’èª­ã¿è¾¼ã¿
2. GPU 0ã§å‹•ç”»åˆ†å‰²ï¼ˆ2ç§’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã€1ç§’ã‚¹ãƒ©ã‚¤ãƒ‰ã€FPS 25â†’20ã€PTå½¢å¼ã€(4, 150, 1, 64, 64)ï¼‰
3. GPU 1ã§éŸ³å£°åˆ†å‰²ï¼ˆ2ç§’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã€1ç§’ã‚¹ãƒ©ã‚¤ãƒ‰ï¼‰
4. GPU 1ã§Whisperæ–‡å­—èµ·ã“ã—ï¼ˆã‚«ã‚¿ã‚«ãƒŠå¤‰æ›ï¼‰
5. æœ€çµ‚CSVã‚’ç”Ÿæˆï¼ˆtrain/validåˆ†å‰²ï¼‰
"""

import os
import subprocess
import time
import sys
import json
import re
import cv2
import numpy as np
from pathlib import Path
from threading import Thread
import logging
import argparse
import pandas as pd
from tqdm import tqdm

# PyTorché–¢é€£
try:
    import torch
    import torchaudio
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸  PyTorchãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")

# Whisperé–¢é€£ï¼ˆæ–‡å­—èµ·ã“ã—ç”¨ï¼‰
try:
    import whisper
    import MeCab
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    print("âš ï¸  Whisper/MeCabãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")

# ========================================
# å‹•ç”»åˆ†å‰²ã‚¯ãƒ©ã‚¹ï¼ˆçµ±åˆï¼‰
# ========================================

class VideoSegmenter:
    """å‹•ç”»åˆ†å‰²ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config):
        self.window_sec = config.get('window_sec', 2.0)  # 2ç§’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
        self.slide_sec = config.get('slide_sec', 1.0)   # 1ç§’ã‚¹ãƒ©ã‚¤ãƒ‰
        self.target_fps = config.get('target_fps', 15)   # ç›®æ¨™FPS
        self.target_size = config.get('target_size', (64, 64))  # ãƒªã‚µã‚¤ã‚ºã‚µã‚¤ã‚º
        
        self.setup_logging()
    
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        self.logger = logging.getLogger(__name__)
    
    def process_single_video(self, video_path: str, output_dir: str, video_name: str):
        """å˜ä¸€å‹•ç”»ã‚’å‡¦ç†"""
        try:
            cap = cv2.VideoCapture(video_path)
            
            if not cap.isOpened():
                self.logger.error(f"å‹•ç”»ã‚’é–‹ã‘ã¾ã›ã‚“: {video_path}")
                return []
            
            # å‹•ç”»æƒ…å ±å–å¾—
            original_fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = total_frames / original_fps
            
            # FPSå¤‰æ›ã®è¨ˆç®—
            fps_ratio = self.target_fps / original_fps
            
            # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã¨ã‚¹ãƒ©ã‚¤ãƒ‰ã®ãƒ•ãƒ¬ãƒ¼ãƒ æ•°
            window_frames = int(self.window_sec * self.target_fps)  # 2ç§’ * 20fps = 40ãƒ•ãƒ¬ãƒ¼ãƒ 
            slide_frames = int(self.slide_sec * self.target_fps)    # 1ç§’ * 20fps = 20ãƒ•ãƒ¬ãƒ¼ãƒ 
            
            segments = []
            segment_idx = 0
            
            # ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§å‡¦ç†
            start_frame_target = 0
            
            while True:
                # å…ƒã®å‹•ç”»ã§ã®é–‹å§‹ãƒ•ãƒ¬ãƒ¼ãƒ ä½ç½®
                start_frame_original = int(start_frame_target / fps_ratio)
                
                if start_frame_original >= total_frames:
                    break
                
                # ãƒ•ãƒ¬ãƒ¼ãƒ åé›†
                frames = []
                
                for i in range(window_frames):
                    frame_idx_original = int((start_frame_target + i) / fps_ratio)
                    
                    if frame_idx_original >= total_frames:
                        break
                    
                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx_original)
                    ret, frame = cap.read()
                    
                    if not ret:
                        break
                    
                    # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
                    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    
                    # ãƒªã‚µã‚¤ã‚º
                    resized = cv2.resize(gray_frame, self.target_size)
                    
                    frames.append(resized)
                
                # ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãƒã‚§ãƒƒã‚¯
                if len(frames) < window_frames * 0.8:  # 80%æœªæº€ãªã‚‰ç ´æ£„
                    break
                
                # ä¸è¶³åˆ†ã‚’ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                while len(frames) < window_frames:
                    frames.append(np.zeros(self.target_size, dtype=np.uint8))
                
                # ãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›: (T, H, W) â†’ (B, T, C, H, W)
                # (40, 64, 64) â†’ (1, 40, 1, 64, 64)
                frames_array = np.array(frames)  # (40, 64, 64)
                
                # (40, 64, 64) â†’ (40, 1, 64, 64) ãƒãƒ£ãƒ³ãƒãƒ«æ¬¡å…ƒè¿½åŠ 
                frames_array = frames_array[:, np.newaxis, :, :]  # (40, 1, 64, 64)
                
                # ãƒãƒƒãƒæ¬¡å…ƒè¿½åŠ : (40, 1, 64, 64) â†’ (1, 40, 1, 64, 64)
                frames_array = frames_array[np.newaxis, :, :, :, :]  # (1, 40, 1, 64, 64)
                
                # PyTorchãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
                tensor = torch.from_numpy(frames_array).float()
                
                # æ­£è¦åŒ– [0, 255] â†’ [0, 1]
                tensor = tensor / 255.0
                
                # ä¿å­˜
                output_path = Path(output_dir) / f"{video_name}_{segment_idx:04d}.pt"
                torch.save(tensor, output_path)
                
                segments.append({
                    'video_name': video_name,
                    'segment_id': segment_idx,
                    'start_time': start_frame_target / self.target_fps,
                    'tensor_path': str(output_path),
                    'tensor_shape': tuple(tensor.shape)
                })
                
                segment_idx += 1
                start_frame_target += slide_frames
            
            cap.release()
            
            return segments
            
        except Exception as e:
            self.logger.error(f"å‹•ç”»å‡¦ç†ã‚¨ãƒ©ãƒ¼ {video_path}: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return []
    
    def process_from_csv(self, csv_path: str, output_dir: str):
        """CSVã‹ã‚‰å‹•ç”»ã‚’ä¸€æ‹¬å‡¦ç†"""
        df = pd.read_csv(csv_path)
        
        if 'video_path' not in df.columns:
            self.logger.error("CSVã«'video_path'åˆ—ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        all_segments = []
        
        print(f"\nå‹•ç”»åˆ†å‰²é–‹å§‹: {len(df)}å€‹ã®å‹•ç”»")
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="å‹•ç”»åˆ†å‰²ä¸­"):
            video_path = row['video_path']
            video_name = row.get('name', Path(video_path).stem)
            
            if not Path(video_path).exists():
                self.logger.warning(f"å‹•ç”»ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {video_path}")
                continue
            
            segments = self.process_single_video(video_path, output_dir, video_name)
            all_segments.extend(segments)
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’CSVã«ä¿å­˜
        if all_segments:
            segments_df = pd.DataFrame(all_segments)
            segments_csv = output_path / 'segments_info.csv'
            segments_df.to_csv(segments_csv, index=False)
            
            self.logger.info(f"å‹•ç”»åˆ†å‰²å®Œäº†: {len(all_segments)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç”Ÿæˆ")
            self.logger.info(f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±: {segments_csv}")
        
        return all_segments

# ========================================
# éŸ³å£°åˆ†å‰²ã‚¯ãƒ©ã‚¹ï¼ˆæ–°è¦è¿½åŠ ï¼‰
# ========================================

class AudioSegmenter:
    """éŸ³å£°åˆ†å‰²ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config):
        self.window_sec = config.get('window_sec', 2.0)  # 2ç§’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
        self.slide_sec = config.get('slide_sec', 1.0)   # 1ç§’ã‚¹ãƒ©ã‚¤ãƒ‰
        self.target_sample_rate = config.get('target_sample_rate', 16000)  # 16kHz
        
        self.setup_logging()
    
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        self.logger = logging.getLogger(__name__)
    
    def process_single_audio(self, audio_path: str, output_dir: str, audio_name: str):
        """å˜ä¸€éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"""
        try:
            # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            waveform, sample_rate = torchaudio.load(audio_path)
            
            # ãƒ¢ãƒãƒ©ãƒ«å¤‰æ›
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
            
            # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            if sample_rate != self.target_sample_rate:
                resampler = torchaudio.transforms.Resample(
                    orig_freq=sample_rate,
                    new_freq=self.target_sample_rate
                )
                waveform = resampler(waveform)
            
            # éŸ³å£°ã®é•·ã•ï¼ˆç§’ï¼‰
            total_samples = waveform.shape[1]
            duration = total_samples / self.target_sample_rate
            
            # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã¨ã‚¹ãƒ©ã‚¤ãƒ‰ã®ã‚µãƒ³ãƒ—ãƒ«æ•°
            window_samples = int(self.window_sec * self.target_sample_rate)
            slide_samples = int(self.slide_sec * self.target_sample_rate)
            
            segments = []
            segment_idx = 0
            
            # ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§å‡¦ç†
            start_sample = 0
            
            while start_sample < total_samples:
                end_sample = start_sample + window_samples
                
                # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ‡ã‚Šå‡ºã—
                if end_sample <= total_samples:
                    segment_waveform = waveform[:, start_sample:end_sample]
                else:
                    # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼šã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                    segment_waveform = waveform[:, start_sample:]
                    padding_size = window_samples - segment_waveform.shape[1]
                    
                    if padding_size > 0:
                        padding = torch.zeros(1, padding_size)
                        segment_waveform = torch.cat([segment_waveform, padding], dim=1)
                
                # 80%æœªæº€ã®é•·ã•ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
                actual_samples = min(end_sample, total_samples) - start_sample
                if actual_samples < window_samples * 0.8:
                    break
                
                # WAVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
                output_path = Path(output_dir) / f"{audio_name}_{segment_idx:04d}.wav"
                torchaudio.save(
                    str(output_path),
                    segment_waveform,
                    self.target_sample_rate
                )
                
                segments.append({
                    'audio_name': audio_name,
                    'segment_id': segment_idx,
                    'start_time': start_sample / self.target_sample_rate,
                    'end_time': min(end_sample, total_samples) / self.target_sample_rate,
                    'audio_path': str(output_path),
                    'duration': segment_waveform.shape[1] / self.target_sample_rate
                })
                
                segment_idx += 1
                start_sample += slide_samples
            
            return segments
            
        except Exception as e:
            self.logger.error(f"éŸ³å£°å‡¦ç†ã‚¨ãƒ©ãƒ¼ {audio_path}: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return []
    
    def process_from_csv(self, csv_path: str, output_dir: str):
        """CSVã‹ã‚‰éŸ³å£°ã‚’ä¸€æ‹¬å‡¦ç†"""
        df = pd.read_csv(csv_path)
        
        if 'audio_path' not in df.columns:
            self.logger.error("CSVã«'audio_path'åˆ—ãŒã‚ã‚Šã¾ã›ã‚“")
            return []
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        all_segments = []
        
        print(f"\néŸ³å£°åˆ†å‰²é–‹å§‹: {len(df)}å€‹ã®éŸ³å£°")
        
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="éŸ³å£°åˆ†å‰²ä¸­"):
            audio_path = row['audio_path']
            audio_name = row.get('name', Path(audio_path).stem)
            
            if not Path(audio_path).exists():
                self.logger.warning(f"éŸ³å£°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {audio_path}")
                continue
            
            segments = self.process_single_audio(audio_path, output_dir, audio_name)
            all_segments.extend(segments)
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’CSVã«ä¿å­˜
        if all_segments:
            segments_df = pd.DataFrame(all_segments)
            segments_csv = output_path / 'audio_segments_info.csv'
            segments_df.to_csv(segments_csv, index=False)
            
            self.logger.info(f"éŸ³å£°åˆ†å‰²å®Œäº†: {len(all_segments)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç”Ÿæˆ")
            self.logger.info(f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±: {segments_csv}")
        
        return all_segments

# ========================================
# æ–‡å­—èµ·ã“ã—ã‚¯ãƒ©ã‚¹ï¼ˆçµ±åˆï¼‰
# ========================================

class AudioTranscriber:
    """éŸ³å£°æ–‡å­—èµ·ã“ã—ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config):
        self.whisper_model_name = config['whisper_model']
        self.output_format = config.get('output_format', 'json')
        self.debug_mode = config.get('debug_mode', False)
        
        self.setup_logging()
        self.load_whisper_model()
        self.setup_katakana_converter()
    
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        self.logger = logging.getLogger(__name__)
    
    def load_whisper_model(self):
        """Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            self.logger.info(f"Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {self.whisper_model_name}")
            self.whisper_model = whisper.load_model(self.whisper_model_name)
            self.logger.info("âœ… Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
        except Exception as e:
            self.logger.error(f"âŒ Whisperãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.whisper_model = None
    
    def setup_katakana_converter(self):
        """ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›è¨­å®š"""
        try:
            dict_paths = [
                '-d /var/lib/mecab/dic/debian',
                '-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd',
                ''
            ]
            
            self.mecab = None
            for dict_path in dict_paths:
                try:
                    self.mecab = MeCab.Tagger(dict_path)
                    self.logger.info(f"âœ… MeCabåˆæœŸåŒ–å®Œäº†: {dict_path if dict_path else 'default'}")
                    break
                except:
                    continue
            
            if self.mecab is None:
                self.mecab = MeCab.Tagger()
                self.logger.info("âœ… MeCabåˆæœŸåŒ–å®Œäº†: default")
                
        except Exception as e:
            self.logger.error(f"âŒ MeCabåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            self.mecab = None
        
        self.hiragana_to_katakana = str.maketrans(
            'ã‚ã„ã†ãˆãŠã‹ããã‘ã“ãŒããã’ã”ã–ã—ã™ã›ãã–ã˜ãšãœããŸã¡ã¤ã¦ã¨ã ã¢ã¥ã§ã©ãªã«ã¬ã­ã®ã¯ã²ãµã¸ã»ã°ã³ã¶ã¹ã¼ã±ã´ã·ãºã½ã¾ã¿ã‚€ã‚ã‚‚ã‚„ã‚†ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚ã‚’ã‚“ã‚ƒã‚…ã‚‡ã£ãƒ¼',
            'ã‚¢ã‚¤ã‚¦ã‚¨ã‚ªã‚«ã‚­ã‚¯ã‚±ã‚³ã‚¬ã‚­ã‚°ã‚²ã‚´ã‚¶ã‚·ã‚¹ã‚»ã‚½ã‚¶ã‚¸ã‚ºã‚¼ã‚¾ã‚¿ãƒãƒ„ãƒ†ãƒˆãƒ€ãƒ‚ãƒ…ãƒ‡ãƒ‰ãƒŠãƒ‹ãƒŒãƒãƒãƒãƒ’ãƒ•ãƒ˜ãƒ›ãƒãƒ“ãƒ–ãƒ™ãƒœãƒ‘ãƒ”ãƒ—ãƒšãƒãƒãƒŸãƒ ãƒ¡ãƒ¢ãƒ¤ãƒ¦ãƒ¨ãƒ©ãƒªãƒ«ãƒ¬ãƒ­ãƒ¯ãƒ²ãƒ³ãƒ£ãƒ¥ãƒ§ãƒƒãƒ¼'
        )
    
    def word_to_katakana(self, word: str) -> str:
        """å˜èªã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›"""
        if not word or self.mecab is None:
            return word.translate(self.hiragana_to_katakana)
        
        try:
            node = self.mecab.parseToNode(word)
            katakana_word = ""
            
            while node:
                surface = node.surface
                features = node.feature.split(',')
                
                if surface == "ã‚’":
                    katakana_word += "ãƒ²"
                elif surface == "ã¯" and len(features) > 0 and features[0] == "åŠ©è©":
                    katakana_word += "ãƒ"
                elif len(features) > 7 and features[7] != '*':
                    katakana_word += features[7]
                elif len(features) > 6 and features[6] != '*':
                    katakana_word += features[6]
                else:
                    katakana_word += surface.translate(self.hiragana_to_katakana)
                
                node = node.next
            
            return katakana_word
        except Exception as e:
            return word.translate(self.hiragana_to_katakana)
    
    def convert_to_katakana_with_mecab(self, text: str) -> str:
        """MeCabã§ã‚«ã‚¿ã‚«ãƒŠå¤‰æ›"""
        if not text or self.mecab is None:
            return text.translate(self.hiragana_to_katakana)
        
        try:
            words = text.split()
            katakana_words = [self.word_to_katakana(word) for word in words]
            return ''.join(katakana_words)
        except Exception as e:
            return text.translate(self.hiragana_to_katakana)
    
    def clean_and_convert_to_katakana(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›"""
        if not text:
            return ""
        
        text = text.strip()
        text = re.sub(r'[\n\t\r]+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[ã€‚ã€ï¼ï¼Ÿï¼ï¼Œ!?.,]', '', text)
        text = self.convert_to_katakana_with_mecab(text)
        text = text.translate(self.hiragana_to_katakana)
        text = text.translate(str.maketrans(
            'ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½š',
            '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'
        ))
        text = re.sub(r'[^\u30A0-\u30FFãƒ¼ãƒ»]', '', text)
        text = re.sub(r'[ãƒ¼ãƒ¼]+', 'ãƒ¼', text)
        text = re.sub(r'[ãƒ»]', '', text)
        
        return text.strip()
    
    def transcribe_single_audio(self, audio_path: str) -> str:
        """å˜ä¸€éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®æ–‡å­—èµ·ã“ã—"""
        if self.whisper_model is None:
            return ""
        
        try:
            result = self.whisper_model.transcribe(
                audio_path,
                language='ja',
                verbose=False,
                fp16=torch.cuda.is_available()
            )
            
            raw_text = result.get('text', '').strip()
            clean_text = self.clean_and_convert_to_katakana(raw_text)
            
            if self.debug_mode and not hasattr(self, 'debug_count'):
                self.debug_count = 0
            
            if self.debug_mode:
                self.debug_count += 1
                if self.debug_count <= 3:
                    self.logger.info(f"å¤‰æ›ä¾‹ {self.debug_count}: '{raw_text}' -> '{clean_text}'")
            
            return clean_text
        except Exception as e:
            self.logger.warning(f"æ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼ {audio_path}: {e}")
            return ""
    
    def find_audio_files(self, audio_dir: str) -> list:
        """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢"""
        audio_path = Path(audio_dir)
        audio_files = []
        
        if audio_path.is_file():
            audio_files = [audio_path]
        elif audio_path.is_dir():
            audio_files = list(audio_path.glob("*.wav"))
            audio_files.extend(audio_path.glob("*.mp3"))
            audio_files.extend(audio_path.glob("*.m4a"))
            audio_files.sort()
        else:
            parent = audio_path.parent
            if parent.exists():
                audio_files = list(parent.rglob("*.wav"))
                audio_files.extend(parent.rglob("*.mp3"))
                audio_files.extend(parent.rglob("*.m4a"))
                audio_files.sort()
        
        return audio_files
    
    def process_audio_directory(self, audio_dir: str, output_file: str):
        """éŸ³å£°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä¸€æ‹¬å‡¦ç†"""
        audio_files = self.find_audio_files(audio_dir)
        
        if not audio_files:
            self.logger.warning(f"éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {audio_dir}")
            return
        
        self.logger.info(f"æ–‡å­—èµ·ã“ã—é–‹å§‹: {len(audio_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        transcription_results = {}
        successful_count = 0
        
        for audio_file in tqdm(audio_files, desc="æ–‡å­—èµ·ã“ã—ä¸­"):
            file_key = audio_file.stem
            transcribed_text = self.transcribe_single_audio(str(audio_file))
            
            transcription_results[file_key] = {
                'audio_file': str(audio_file),
                'transcribed_text': transcribed_text,
                'text_length': len(transcribed_text),
                'has_text': len(transcribed_text) > 0
            }
            
            if transcribed_text:
                successful_count += 1
        
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(transcription_results, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"æ–‡å­—èµ·ã“ã—å®Œäº†: {successful_count}/{len(audio_files)}ãƒ•ã‚¡ã‚¤ãƒ«")

# ========================================
# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆã‚¯ãƒ©ã‚¹
# ========================================

class CompletePipeline:
    """å®Œå…¨çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config):
        self.config = config
        self.input_csv = config['input_csv']
        self.output_dir = Path(config['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.setup_logging()
        
        # å‡¦ç†çŠ¶æ…‹
        self.video_done = False
        self.audio_done = False
        self.transcribe_done = False
        self.video_error = None
        self.audio_error = None
        self.transcribe_error = None
    
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        log_file = self.output_dir / 'process.log'
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(threadName)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def verify_input_csv(self):
        """å…¥åŠ›CSVæ¤œè¨¼"""
        if not Path(self.input_csv).exists():
            self.logger.error(f"å…¥åŠ›CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.input_csv}")
            return False
        
        try:
            df = pd.read_csv(self.input_csv)
            required_cols = ['video_path', 'audio_path']
            missing_cols = [col for col in required_cols if col not in df.columns]
            
            if missing_cols:
                self.logger.error(f"å¿…é ˆåˆ—ãŒä¸è¶³: {missing_cols}")
                return False
            
            self.logger.info(f"å…¥åŠ›CSVæ¤œè¨¼OK: {len(df)}ãƒšã‚¢")
            
            print("\nå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®3ä»¶ï¼‰:")
            for i, row in df.head(3).iterrows():
                print(f"  {i+1}. å‹•ç”»: {Path(row['video_path']).name}")
                print(f"      éŸ³å£°: {Path(row['audio_path']).name}")
            
            return True
        except Exception as e:
            self.logger.error(f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def gpu0_video_processing(self):
        """GPU 0: å‹•ç”»åˆ†å‰²ï¼ˆçµ±åˆå®Ÿè¡Œï¼‰"""
        self.logger.info("[GPU 0] å‹•ç”»åˆ†å‰²å‡¦ç†é–‹å§‹")
        
        try:
            # GPU 0ã‚’æŒ‡å®š
            os.environ['CUDA_VISIBLE_DEVICES'] = '0'
            
            video_output = self.output_dir / 'video_segments'
            video_output.mkdir(parents=True, exist_ok=True)
            
            # VideoSegmenterã§å‡¦ç†
            segmenter_config = {
                'window_sec': self.config.get('window_sec', 2.0),
                'slide_sec': self.config.get('slide_sec', 1.0),
                'target_fps': self.config.get('target_fps', 20),
                'target_size': (64, 64)
            }
            
            segmenter = VideoSegmenter(segmenter_config)
            segments = segmenter.process_from_csv(self.input_csv, str(video_output))
            
            self.logger.info(f"[GPU 0] å‹•ç”»åˆ†å‰²å®Œäº†: {len(segments) if segments else 0}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
            self.video_done = True
            
        except Exception as e:
            self.logger.error(f"[GPU 0] ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            self.video_error = str(e)
    
    def gpu1_audio_and_transcribe(self):
        """GPU 1: éŸ³å£°åˆ†å‰² + æ–‡å­—èµ·ã“ã—ï¼ˆçµ±åˆå®Ÿè¡Œï¼‰"""
        self.logger.info("[GPU 1] éŸ³å£°å‡¦ç† + æ–‡å­—èµ·ã“ã—é–‹å§‹")
        
        # GPU 1ã‚’æŒ‡å®š
        os.environ['CUDA_VISIBLE_DEVICES'] = '1'
        
        try:
            # ã‚¹ãƒ†ãƒƒãƒ—1: éŸ³å£°åˆ†å‰²
            self.logger.info("[GPU 1] éŸ³å£°åˆ†å‰²å‡¦ç†é–‹å§‹")
            audio_output = self.output_dir / 'audio_segments'
            audio_output.mkdir(parents=True, exist_ok=True)
            
            # AudioSegmenterã§å‡¦ç†
            audio_segmenter_config = {
                'window_sec': self.config.get('window_sec', 2.0),
                'slide_sec': self.config.get('slide_sec', 1.0),
                'target_sample_rate': 16000
            }
            
            audio_segmenter = AudioSegmenter(audio_segmenter_config)
            audio_segments = audio_segmenter.process_from_csv(self.input_csv, str(audio_output))
            
            self.logger.info(f"[GPU 1] éŸ³å£°åˆ†å‰²å®Œäº†: {len(audio_segments) if audio_segments else 0}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
            self.audio_done = True
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: æ–‡å­—èµ·ã“ã—ï¼ˆçµ±åˆå®Ÿè¡Œï¼‰
            self.logger.info("[GPU 1] æ–‡å­—èµ·ã“ã—å‡¦ç†é–‹å§‹")
            transcription_file = self.output_dir / 'transcriptions.json'
            
            transcriber_config = {
                'whisper_model': self.config.get('whisper_model', 'base'),
                'output_format': 'json',
                'debug_mode': self.config.get('debug_mode', False)
            }
            
            transcriber = AudioTranscriber(transcriber_config)
            transcriber.process_audio_directory(str(audio_output), str(transcription_file))
            
            self.logger.info("[GPU 1] æ–‡å­—èµ·ã“ã—å®Œäº†")
            self.transcribe_done = True
            
        except Exception as e:
            self.logger.error(f"[GPU 1] äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            
            if not self.audio_done:
                self.audio_error = str(e)
            else:
                self.transcribe_error = str(e)
    
    def combine_to_final_csv(self):
        """æœ€çµ‚CSVç”Ÿæˆï¼ˆçµ±åˆå®Ÿè¡Œï¼‰"""
        self.logger.info("æœ€çµ‚CSVç”Ÿæˆé–‹å§‹")
        
        try:
            video_dir = self.output_dir / 'video_segments'
            transcription_file = self.output_dir / 'transcriptions.json'
            
            # æ–‡å­—èµ·ã“ã—çµæœã‚’èª­ã¿è¾¼ã¿
            if not transcription_file.exists():
                self.logger.error(f"æ–‡å­—èµ·ã“ã—çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {transcription_file}")
                return False
            
            with open(transcription_file, 'r', encoding='utf-8') as f:
                transcriptions = json.load(f)
            
            # å‹•ç”»ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
            video_files = list(video_dir.glob("*.pt"))
            
            if not video_files:
                self.logger.error(f"å‹•ç”»ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {video_dir}")
                return False
            
            self.logger.info(f"å‹•ç”»ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ: {len(video_files)}å€‹")
            self.logger.info(f"æ–‡å­—èµ·ã“ã—çµæœ: {len(transcriptions)}å€‹")
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            dataset = []
            
            for video_file in tqdm(video_files, desc="ãƒ‡ãƒ¼ã‚¿çµåˆä¸­"):
                video_name = video_file.stem  # æ‹¡å¼µå­ãªã—ã®ãƒ•ã‚¡ã‚¤ãƒ«å
                
                # å¯¾å¿œã™ã‚‹æ–‡å­—èµ·ã“ã—ã‚’æ¢ã™
                # video_nameå½¢å¼: "video1_0001" â†’ å…ƒã®åå‰ "video1" ã‚’æŠ½å‡º
                base_name = '_'.join(video_name.split('_')[:-1])  # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç•ªå·ã‚’é™¤å»
                
                # å®Œå…¨ä¸€è‡´ã‚’å„ªå…ˆ
                transcription = transcriptions.get(video_name, None)
                
                # å®Œå…¨ä¸€è‡´ãŒãªã„å ´åˆã€ãƒ™ãƒ¼ã‚¹åã§æ¤œç´¢
                if transcription is None:
                    transcription = transcriptions.get(base_name, None)
                
                if transcription and transcription.get('has_text', False):
                    text = transcription['transcribed_text']
                    text_len = transcription['text_length']
                    
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    min_len = self.config.get('min_text_len', 3)
                    max_len = self.config.get('max_text_len', 50)
                    
                    if min_len <= text_len <= max_len:
                        dataset.append({
                            'video_path': str(video_file),
                            'text': text,
                            'text_length': text_len
                        })
            
            if not dataset:
                self.logger.error("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒ1ä»¶ã‚‚ã‚ã‚Šã¾ã›ã‚“")
                return False
            
            self.logger.info(f"æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿: {len(dataset)}ä»¶")
            
            # DataFrameã«å¤‰æ›
            df = pd.DataFrame(dataset)
            
            # ã‚·ãƒ£ãƒƒãƒ•ãƒ«
            df = df.sample(frac=1, random_state=42).reset_index(drop=True)
            
            # train/validåˆ†å‰²
            valid_ratio = self.config.get('valid_ratio', 0.2)
            valid_size = int(len(df) * valid_ratio)
            train_size = len(df) - valid_size
            
            df_train = df.iloc[:train_size]
            df_valid = df.iloc[train_size:]
            
            # CSVä¿å­˜
            train_csv = self.output_dir / 'final_train.csv'
            valid_csv = self.output_dir / 'final_valid.csv'
            
            df_train.to_csv(train_csv, index=False, encoding='utf-8')
            df_valid.to_csv(valid_csv, index=False, encoding='utf-8')
            
            self.logger.info(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(df_train)}ä»¶ â†’ {train_csv}")
            self.logger.info(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(df_valid)}ä»¶ â†’ {valid_csv}")
            
            # çµ±è¨ˆæƒ…å ±
            print("\n" + "="*70)
            print("=== æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ ===")
            print("="*70)
            print(f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}ä»¶")
            print(f"å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(df_train)}ä»¶ ({len(df_train)/len(df)*100:.1f}%)")
            print(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(df_valid)}ä»¶ ({len(df_valid)/len(df)*100:.1f}%)")
            print(f"\nãƒ†ã‚­ã‚¹ãƒˆé•·çµ±è¨ˆ:")
            print(f"  å¹³å‡: {df['text_length'].mean():.1f}æ–‡å­—")
            print(f"  æœ€å°: {df['text_length'].min()}æ–‡å­—")
            print(f"  æœ€å¤§: {df['text_length'].max()}æ–‡å­—")
            print(f"  ä¸­å¤®å€¤: {df['text_length'].median():.1f}æ–‡å­—")
            print("="*70)
            
            # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            print("\n=== ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰5ä»¶ï¼‰===")
            for idx, row in df_train.head(5).iterrows():
                print(f"{idx+1}. {Path(row['video_path']).name}")
                print(f"   ãƒ†ã‚­ã‚¹ãƒˆ: {row['text']} ({row['text_length']}æ–‡å­—)")
            print("="*70)
            
            return True
            
        except Exception as e:
            self.logger.error(f"CSVç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return False
    
    def monitor_progress(self):
        """é€²æ—ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°"""
        print("\n" + "="*70)
        print("=== å‡¦ç†é€²æ— ===")
        print("="*70)
        print("GPU 0: å‹•ç”»åˆ†å‰²ï¼ˆ2ç§’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã€1ç§’ã‚¹ãƒ©ã‚¤ãƒ‰ã€FPS20ã€PTå½¢å¼ï¼‰")
        print("GPU 1: éŸ³å£°åˆ†å‰²ï¼ˆ2ç§’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã€1ç§’ã‚¹ãƒ©ã‚¤ãƒ‰ï¼‰+ Whisperæ–‡å­—èµ·ã“ã—")
        print("="*70)
        print()
        
        start_time = time.time()
        
        while not (self.video_done and self.audio_done and self.transcribe_done):
            elapsed = time.time() - start_time
            mins = int(elapsed // 60)
            secs = int(elapsed % 60)
            
            video_status = "âœ… å®Œäº†" if self.video_done else ("âŒ ã‚¨ãƒ©ãƒ¼" if self.video_error else "ğŸ”„ å‡¦ç†ä¸­")
            audio_status = "âœ… å®Œäº†" if self.audio_done else ("âŒ ã‚¨ãƒ©ãƒ¼" if self.audio_error else "ğŸ”„ å‡¦ç†ä¸­")
            transcribe_status = "âœ… å®Œäº†" if self.transcribe_done else ("âŒ ã‚¨ãƒ©ãƒ¼" if self.transcribe_error else ("â¸ï¸  å¾…æ©Ÿä¸­" if not self.audio_done else "ğŸ”„ å‡¦ç†ä¸­"))
            
            print(f"\rçµŒéæ™‚é–“: {mins:02d}:{secs:02d} | "
                  f"å‹•ç”»: {video_status:12s} | "
                  f"éŸ³å£°: {audio_status:12s} | "
                  f"æ–‡å­—èµ·ã“ã—: {transcribe_status:12s}",
                  end='', flush=True)
            
            if self.video_error or self.audio_error or self.transcribe_error:
                print("\n\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                return False
            
            time.sleep(1)
        
        elapsed = time.time() - start_time
        print(f"\n\nâœ… ä¸¦åˆ—å‡¦ç†å®Œäº†ï¼ å‡¦ç†æ™‚é–“: {elapsed/60:.1f}åˆ†\n")
        return True
    
    def run(self):
        """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("\n" + "="*70)
        print("=== èª­å”‡è¡“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Œå…¨è‡ªå‹•å‡¦ç† ===")
        print("="*70)
        print(f"å…¥åŠ›CSV: {self.input_csv}")
        print(f"å‡ºåŠ›å…ˆ: {self.output_dir}")
        print(f"å‹•ç”»è¨­å®š: {self.config.get('window_sec', 2.0)}ç§’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã€{self.config.get('slide_sec', 1.0)}ç§’ã‚¹ãƒ©ã‚¤ãƒ‰ã€FPS{self.config.get('target_fps', 20)}")
        print(f"éŸ³å£°è¨­å®š: {self.config.get('window_sec', 2.0)}ç§’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã€{self.config.get('slide_sec', 1.0)}ç§’ã‚¹ãƒ©ã‚¤ãƒ‰ã€16kHz")
        print(f"ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶: (1, 40, 1, 64, 64)")
        print(f"Whisperãƒ¢ãƒ‡ãƒ«: {self.config.get('whisper_model', 'base')}")
        print("="*70)
        
        overall_start = time.time()
        
        if not self.verify_input_csv():
            return False
        
        print("\nå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
        time.sleep(2)
        
        video_thread = Thread(target=self.gpu0_video_processing, name="GPU-0")
        audio_thread = Thread(target=self.gpu1_audio_and_transcribe, name="GPU-1")
        
        video_thread.start()
        audio_thread.start()
        
        if not self.monitor_progress():
            video_thread.join()
            audio_thread.join()
            return False
        
        video_thread.join()
        audio_thread.join()
        
        print("="*70)
        print("=== æœ€çµ‚CSVç”Ÿæˆ ===")
        print("="*70)
        
        if not self.combine_to_final_csv():
            return False
        
        overall_elapsed = time.time() - overall_start
        
        print("\n" + "="*70)
        print("=== ğŸ‰ å…¨å‡¦ç†å®Œäº†ï¼ ===")
        print("="*70)
        print(f"ç·å‡¦ç†æ™‚é–“: {overall_elapsed/60:.1f}åˆ†")
        print(f"\nğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"   å‹•ç”»ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ: {self.output_dir / 'video_segments'}")
        print(f"   éŸ³å£°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ: {self.output_dir / 'audio_segments'}")
        print(f"   æ–‡å­—èµ·ã“ã—çµæœ: {self.output_dir / 'transcriptions.json'}")
        print(f"   æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {self.output_dir / 'final_train.csv'}")
        print(f"                     {self.output_dir / 'final_valid.csv'}")
        print(f"\nğŸ“Š ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«:")
        print(f"   {self.output_dir / 'process.log'}")
        print("="*70)
        
        return True

# ========================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ========================================

def main():
    # ========================================
    # ğŸ”§ è¨­å®šï¼ˆã“ã“ã‚’ç·¨é›†ï¼‰
    # ========================================
    
    # å…¥åŠ›CSVï¼ˆcreate_dataset_csv.pyã§ä½œæˆã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
    INPUT_CSV = '/home/bv20049/dataset/npz/zundadata/dataset_be.csv'
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    OUTPUT_DIR = '/home/bv20049/dataset/npz/zundadata/processed'
    
    # Whisperãƒ¢ãƒ‡ãƒ«ï¼ˆtiny, base, small, medium, largeï¼‰
    WHISPER_MODEL = 'medium'
    
    # å‹•ç”»ãƒ»éŸ³å£°å‡¦ç†è¨­å®š
    WINDOW_SEC = 2.0        # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼ˆç§’ï¼‰
    SLIDE_SEC = 1.0         # ã‚¹ãƒ©ã‚¤ãƒ‰å¹…ï¼ˆç§’ï¼‰
    TARGET_FPS = 20         # ç›®æ¨™FPSï¼ˆå‹•ç”»ã®ã¿ï¼‰
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š
    MIN_TEXT_LENGTH = 3     # æœ€å°ãƒ†ã‚­ã‚¹ãƒˆé•·
    MAX_TEXT_LENGTH = 50    # æœ€å¤§ãƒ†ã‚­ã‚¹ãƒˆé•·
    VALID_RATIO = 0.2       # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿å‰²åˆï¼ˆ20%ï¼‰
    
    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
    DEBUG_MODE = False
    
    # ========================================
    
    parser = argparse.ArgumentParser(description='èª­å”‡è¡“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Œå…¨è‡ªå‹•å‡¦ç†')
    parser.add_argument('--input', help='å…¥åŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--output', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--whisper-model', choices=['tiny', 'base', 'small', 'medium', 'large'])
    parser.add_argument('--window-sec', type=float, help='ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºï¼ˆç§’ï¼‰')
    parser.add_argument('--slide-sec', type=float, help='ã‚¹ãƒ©ã‚¤ãƒ‰å¹…ï¼ˆç§’ï¼‰')
    parser.add_argument('--target-fps', type=int, help='ç›®æ¨™FPS')
    parser.add_argument('--min-text-len', type=int)
    parser.add_argument('--max-text-len', type=int)
    parser.add_argument('--valid-ratio', type=float)
    parser.add_argument('--debug', action='store_true')
    
    args = parser.parse_args()
    
    config = {
        'input_csv': args.input if args.input else INPUT_CSV,
        'output_dir': args.output if args.output else OUTPUT_DIR,
        'whisper_model': args.whisper_model if args.whisper_model else WHISPER_MODEL,
        'window_sec': args.window_sec if args.window_sec else WINDOW_SEC,
        'slide_sec': args.slide_sec if args.slide_sec else SLIDE_SEC,
        'target_fps': args.target_fps if args.target_fps else TARGET_FPS,
        'min_text_len': args.min_text_len if args.min_text_len else MIN_TEXT_LENGTH,
        'max_text_len': args.max_text_len if args.max_text_len else MAX_TEXT_LENGTH,
        'valid_ratio': args.valid_ratio if args.valid_ratio else VALID_RATIO,
        'debug_mode': args.debug or DEBUG_MODE
    }
    
    print("\n" + "="*70)
    print("=== è¨­å®šç¢ºèª ===")
    print("="*70)
    print(f"å…¥åŠ›CSV: {config['input_csv']}")
    print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {config['output_dir']}")
    print(f"\nå‹•ç”»å‡¦ç†è¨­å®š:")
    print(f"  ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º: {config['window_sec']}ç§’")
    print(f"  ã‚¹ãƒ©ã‚¤ãƒ‰å¹…: {config['slide_sec']}ç§’")
    print(f"  ç›®æ¨™FPS: {config['target_fps']}")
    print(f"  å‡ºåŠ›å½¢çŠ¶: (1, 40, 1, 64, 64)")
    print(f"\néŸ³å£°å‡¦ç†è¨­å®š:")
    print(f"  ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º: {config['window_sec']}ç§’")
    print(f"  ã‚¹ãƒ©ã‚¤ãƒ‰å¹…: {config['slide_sec']}ç§’")
    print(f"  ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ: 16kHz")
    print(f"  Whisperãƒ¢ãƒ‡ãƒ«: {config['whisper_model']}")
    print(f"\nãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š:")
    print(f"  ãƒ†ã‚­ã‚¹ãƒˆé•·: {config['min_text_len']}ï½{config['max_text_len']}æ–‡å­—")
    print(f"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿å‰²åˆ: {config['valid_ratio']*100:.0f}%")
    print(f"\nãã®ä»–:")
    print(f"  ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰: {'ON' if config['debug_mode'] else 'OFF'}")
    print("="*70)
    
    # ç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    response = input("\nã“ã®è¨­å®šã§å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
    if response.lower() != 'y':
        print("ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
        return
    
    # ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
    if not TORCH_AVAILABLE:
        print("âŒ PyTorch/torchaudioãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install torch torchaudio")
        return
    
    if not WHISPER_AVAILABLE:
        print("âŒ Whisper/MeCabãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install openai-whisper mecab-python3")
        return
    
    # å®Ÿè¡Œ
    pipeline = CompletePipeline(config)
    success = pipeline.run()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()